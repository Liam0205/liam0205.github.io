<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon/favicon-16x16.png">
  <link rel="mask-icon" href="/images/favicon/safari-pinned-tab.svg" color="#222">
  <link rel="manifest" href="/images/favicon/site.webmanifest">
  <meta name="msapplication-config" content="/images/favicon/browserconfig.xml">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <meta name="google-site-verification" content="TYuJQFuiVqfbFfJAkHfXigSDRIMibztxXxgl_iAaQhA">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"liam.page","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="早在学习数学分析时，我就已经接触过卷积的概念。然而，彼时年少，水平有限，没有完整地理解卷积的概念和精髓。这个遗憾一直持续至今。接触到卷积神经网络（Convolution Neural Network, CNN）之后，旧事重提般地，想要了解清楚卷积的冲动就愈发强烈，终至此文。  这是一篇介绍性质的文章。文中的公式、动画效果限于网页的表现力，无法达至完美。本文有对应的 PDF 格式的幻灯片可供下载（离">
<meta property="og:type" content="article">
<meta property="og:title" content="谈谈离散卷积和卷积神经网络">
<meta property="og:url" content="https://liam.page/2017/07/27/convolutions-and-convolution-neural-network/index.html">
<meta property="og:site_name" content="始终">
<meta property="og:description" content="早在学习数学分析时，我就已经接触过卷积的概念。然而，彼时年少，水平有限，没有完整地理解卷积的概念和精髓。这个遗憾一直持续至今。接触到卷积神经网络（Convolution Neural Network, CNN）之后，旧事重提般地，想要了解清楚卷积的冲动就愈发强烈，终至此文。  这是一篇介绍性质的文章。文中的公式、动画效果限于网页的表现力，无法达至完美。本文有对应的 PDF 格式的幻灯片可供下载（离">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://liam.page/uploads/images/MachineLearning/linear_time-invariant_system.png">
<meta property="og:image" content="https://liam.page/uploads/images/MachineLearning/automatic_investment_plan.gif">
<meta property="og:image" content="https://liam.page/uploads/images/MachineLearning/how_to_convolve.png">
<meta property="og:image" content="https://liam.page/uploads/images/MachineLearning/convolutions_on_images.gif">
<meta property="og:image" content="https://liam.page/uploads/images/MachineLearning/image_filter.png">
<meta property="og:image" content="https://liam.page/uploads/images/MachineLearning/pattern_reg_target.png">
<meta property="og:image" content="https://liam.page/uploads/images/MachineLearning/pattern_reg_dnn.png">
<meta property="og:image" content="https://liam.page/uploads/images/MachineLearning/pattern_reg_cases.png">
<meta property="og:image" content="https://liam.page/uploads/images/MachineLearning/pattern_reg_cnn.png">
<meta property="article:published_time" content="2017-07-27T06:17:29.000Z">
<meta property="article:modified_time" content="2020-03-02T14:20:07.000Z">
<meta property="article:author" content="Liam Huang">
<meta property="article:tag" content="Convolutions">
<meta property="article:tag" content="CNN">
<meta property="article:tag" content="Neural Network">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://liam.page/uploads/images/MachineLearning/linear_time-invariant_system.png">

<link rel="canonical" href="https://liam.page/2017/07/27/convolutions-and-convolution-neural-network/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<link rel="stylesheet" href="//cdn.jsdelivr.net/gh/theme-next/theme-next-needmoreshare2@1/needsharebutton.min.css"><style>
#needsharebutton-postbottom {
  cursor: pointer;
  height: 26px;
  margin-top: 10px;
  position: relative;
}
#needsharebutton-postbottom .btn {
  border: 1px solid $btn-default-border-color;
  border-radius: 3px;
  display: initial;
  padding: 1px 4px;
}
</style>
  <title>谈谈离散卷积和卷积神经网络 | 始终</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-44836433-1"></script>
    <script data-pjax>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-44836433-1');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">始终</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">不忘初心</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">404</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">15</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">773</span></a>

  </li>
        <li class="menu-item menu-item-系列">

    <a href="/series/" rel="section"><i class="fa fa-fw fa-book"></i>系列</a>

  </li>
        <li class="menu-item menu-item-札记-&-留言板">

    <a href="/notes/" rel="section"><i class="fa fa-fw fa-sticky-note"></i>札记 & 留言板</a>

  </li>
        <li class="menu-item menu-item-英文">

    <a href="/en/" rel="section"><i class="fa fa-fw fa-link"></i>英文</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://liam.page/2017/07/27/convolutions-and-convolution-neural-network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar/avatar-3.jpg">
      <meta itemprop="name" content="Liam Huang">
      <meta itemprop="description" content="虚室生白，吉祥止止">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="始终">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          谈谈离散卷积和卷积神经网络
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017 年 07 月 27 日 14:17:29" itemprop="dateCreated datePublished" datetime="2017-07-27T14:17:29+08:00">2017 年 07 月 27 日</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020 年 03 月 02 日 22:20:07" itemprop="dateModified" datetime="2020-03-02T22:20:07+08:00">2020 年 03 月 02 日</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Big-Data-and-Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Big Data and Machine Learning</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>11k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>21 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>早在学习数学分析时，我就已经接触过卷积的概念。然而，彼时年少，水平有限，没有完整地理解卷积的概念和精髓。这个遗憾一直持续至今。接触到卷积神经网络（Convolution Neural Network, CNN）之后，旧事重提般地，想要了解清楚卷积的冲动就愈发强烈，终至此文。</p>
<blockquote>
<p>这是一篇介绍性质的文章。文中的公式、动画效果限于网页的表现力，无法达至完美。本文有对应的 PDF 格式的幻灯片可供下载（<a href="//liam.page/attachment/attachment/slides/convolutions.pdf">离散卷积和卷积神经网络</a>）。你可能需要使用 <a target="_blank" rel="noopener" href="https://get.adobe.com/cn/reader/">Adobe Acrobat&#x2F;Reader</a> 作为 PDF 阅读器，以获得幻灯片的所有效果。</p>
</blockquote>
<span id="more"></span>

<h2 id="初识卷积"><a href="#初识卷积" class="headerlink" title="初识卷积"></a>初识卷积</h2><h3 id="一问卷积"><a href="#一问卷积" class="headerlink" title="一问卷积"></a>一问卷积</h3><p>「卷积」这个词给人的第一印象就是「萌萌哒」，因此，恐怕很多人听见卷积的第一反应会是：「卷积<del>可以吃吗</del>为什么要叫这个名字」。</p>
<p>粗暴地回答的话，理由有三：</p>
<ul>
<li>卷在这里对应英文的 convolve 这个单词，在卷积这个概念中，它的本意是「翻转」；</li>
<li>积在这里对应乘积，因为卷积是通过两个函数&#x2F;序列的乘积实现的；</li>
<li>它真的是在「卷」——把多个乘积卷在一起变成一个值。</li>
</ul>
<h3 id="形式定义"><a href="#形式定义" class="headerlink" title="形式定义"></a>形式定义</h3><p>在具体介绍卷积是什么、为什么是这样、有什么用之前，让我们预先「先入为主」地看一看一维卷积的定义是什么样的。</p>
<ul>
<li>离散形式：$(x * y)[n] &#x3D; \sum_{m &#x3D; -\infty}^{+\infty}x[m]\cdot y[n - m]$。</li>
<li>连续形式：$(f * g)(t) &#x3D; \int_{-\infty}^{+\infty}f(\tau)\cdot g(t - \tau)\mathop{}\!\mathrm{d}\tau$。</li>
</ul>
<blockquote>
<p>当然，此篇主要介绍离散卷积，因此连续卷积从这一刻起就被暂时打入冷宫了。</p>
</blockquote>
<p>如果你仔细观察离散卷积的定义，你就会发现，它也可以写成如下等价形式。</p>
<p>$$(x * y)[n] &#x3D; \sum_{i + k &#x3D; n}x[i]\cdot y[k].$$</p>
<p>可以看到，卷积将等式右边的两个变量 $i$, $j$ 变成了等式左边的一个变量 $n$——俗称：降维打击。</p>
<h2 id="一维离散卷积"><a href="#一维离散卷积" class="headerlink" title="一维离散卷积"></a>一维离散卷积</h2><p>在详细介绍一维离散卷积之前，我们需要先了解什么是「线性时不变系统」。此后，在脉冲激励和冲激响应的叠加中，我们就能得到卷积。</p>
<h3 id="线性时不变系统"><a href="#线性时不变系统" class="headerlink" title="线性时不变系统"></a>线性时不变系统</h3><p><img data-src="/uploads/images/MachineLearning/linear_time-invariant_system.png" alt="线性时不变系统"></p>
<p>线性时不变系统（Linear Time-invariant System）是一种特殊的信号系统。它的特性分成「线性」和「时不变」两个维度。</p>
<p>所谓线性，说的是系统的输出对输入满足齐次性和叠加性。这也就是说，若输入 <code>$x_1(\tau)$</code> 和 <code>$x_2(\tau)$</code> 分别得到 <code>$y_1(\tau)$</code> 和 <code>$y_2(\tau)$</code>，那么对于任意的常数 <code>$c_1$</code>, <code>$c_2$</code> 满足 <code>$c_1x_1(\tau) + c_2x_2(\tau)$</code> 的输入在系统的作用下产生输出 <code>$c_1y_1(\tau) + c_2y_2(\tau)$</code>。</p>
<table>
<thead>
<tr>
<th>输入</th>
<th>输出</th>
</tr>
</thead>
<tbody><tr>
<td><code>$x_1(\tau)$</code></td>
<td><code>$y_1(\tau)$</code></td>
</tr>
<tr>
<td><code>$x_2(\tau)$</code></td>
<td><code>$y_2(\tau)$</code></td>
</tr>
<tr>
<td><code>$c_1x_1(\tau) + c_2x_2(\tau)$</code></td>
<td><code>$c_1y_1(\tau) + c_2y_2(\tau)$</code></td>
</tr>
</tbody></table>
<p>所谓时不变，说的是系统对固定输入的输出响应不随时间发生变化。这也就是说，若输入 $x(\tau)$ 得到输出 $y(\tau)$，则若输入 <code>$x(t_0 + \tau)$</code> 得到输出 <code>$y(t_0 + \tau)$</code>。</p>
<table>
<thead>
<tr>
<th>输入</th>
<th>输出</th>
</tr>
</thead>
<tbody><tr>
<td><code>$x(\tau)$</code></td>
<td><code>$y(\tau)$</code></td>
</tr>
<tr>
<td><code>$x(t_0 + \tau)$</code></td>
<td><code>$y(t_0 + \tau)$</code></td>
</tr>
</tbody></table>
<h3 id="冲激和响应"><a href="#冲激和响应" class="headerlink" title="冲激和响应"></a>冲激和响应</h3><p>信号系统的输入，称之为「激励」。对信号系统来说，它通常会接收一连串的激励。这一连串的激励，通常在瞬时发生，然后消退。因此信号系统的输入又称之为「脉冲激励」，简称「冲激」。若以 $y$ 记信号系统的冲激，则它应该是一个序列 $y[n]$。具体的值 $y[i]$ 或 $y_i$ 表示第 $i$ 时刻信号系统接收的脉冲激励。</p>
<p><code>$$y[n] = \&#123; \ldots, y_&#123;-1&#125; = 0, y_0 = i, y_1 = j, y_2 = k, y_3 = 0, \ldots \&#125;.$$</code></p>
<p>信号系统的输出，称之为「响应」。对于单位强度冲激的响应，即是「冲激响应」。信号系统对单独的冲激，做出的响应输出，可能在冲激发生之后持续一段时间。若以 $x$ 记信号系统的冲激响应，则它也应该是一个序列 $x[n]$。具体的值 $x[i]$ 或 $x_i$ 表示系统接收到单位强度冲激之后第 $i$ 时刻做出的响应输出。</p>
<blockquote>
<p>若你不太理解为何响应会在冲激发生之后持续一段时间，那么你可以把自己比作一个信号系统。当你遇到什么开心&#x2F;不开心的事情之后，你高兴&#x2F;伤心的情绪不会只在那一瞬间出现，而是会持续一段时间。</p>
</blockquote>
<p><code>$$x[n] = \&#123; \ldots, x_&#123;-1&#125; = 0, x_0 = a, x_1 = b, x_2 = c, x_3 = 0, \ldots \&#125;.$$</code></p>
<h3 id="连续冲激的响应"><a href="#连续冲激的响应" class="headerlink" title="连续冲激的响应"></a>连续冲激的响应</h3><p>现在我们知道几个事实：</p>
<ul>
<li>系统接收到一份输入后，其后的一段时间内会陆续给出输出响应；</li>
<li>系统会连续收到若干输入；</li>
<li>系统是线性时不变的。</li>
</ul>
<p>特别地，输入冲激 $\hat y$ 在 $x[n]$ 的作用下，第 $i$ 时刻的输出是 $\hat y\cdot x[i]$。因此，整个线性时不变系统在第 $i$ 时刻的输出，应该是<br>$$y[0]\cdot x[i] + y[1]\cdot x[i - 1] + \cdots.$$<br>当然，对于两端延伸的无穷序列，你应该把它写作<br>$$\cdots + y[-1]\cdot x[i + 1] + y[0]\cdot x[i] + y[1]\cdot x[i - 1] + \cdots.$$</p>
<p>以上一小节的数据为例，将数据制成表如下：</p>
<table>
<thead>
<tr>
<th>time</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr>
</thead>
<tbody><tr>
<td>$y_0 &#x3D; i$</td>
<td>$ai$</td>
<td>$bi$</td>
<td>$ci$</td>
<td>$0$</td>
<td>$0$</td>
</tr>
<tr>
<td>$y_1 &#x3D; j$</td>
<td>$0$</td>
<td>$aj$</td>
<td>$bj$</td>
<td>$cj$</td>
<td>$0$</td>
</tr>
<tr>
<td>$y_2 &#x3D; k$</td>
<td>$0$</td>
<td>$0$</td>
<td>$ak$</td>
<td>$bk$</td>
<td>$ck$</td>
</tr>
</tbody></table>
<p>接下来，你只需要纵向观察表格，将每一纵列的值相加，就能得到相应时刻的系统输出响应了。</p>
<h3 id="离散卷积"><a href="#离散卷积" class="headerlink" title="离散卷积"></a>离散卷积</h3><p>从上面的分析中，可以看出，对于任意时刻 $n$，系统的输出是</p>
<p><code>$$z[n] \overset&#123;\text&#123;def&#125;&#125;&#123;=&#125; (x*y)[n] = \sum_&#123;m = -\infty&#125;^&#123;+\infty&#125;x[n - m]\cdot y[m] = \sum_&#123;m = -\infty&#125;^&#123;+\infty&#125;x[m]\cdot y[n - m].$$</code></p>
<p>这正是一维离散卷积的定义。以 <code>$\sum_&#123;m = -\infty&#125;^&#123;+\infty&#125;x[m]\cdot y[n - m]$</code> 为例，不难发现，卷积其实是一种推广的加权平均：以 $x$ 为权，以 $n$ 为中心，把 $y$ 距离中心 $-m$ 位置上的值乘上 $x$ 在 $m$ 位置的值，最后加到一起。</p>
<h3 id="定投的例子"><a href="#定投的例子" class="headerlink" title="定投的例子"></a>定投的例子</h3><p>现在假设有一个一年期定投项目，它的利率始终保持不变。因此，整个定投项目可以视作是一个线性时不变系统。</p>
<ul>
<li>最终收益对投入的资金是线性累加的；</li>
<li>利率不变，意味着任何时候投入资金的效果是一样的。</li>
</ul>
<p>因此，你可以定义响应序列</p>
<p><code>$$x[n] = \&#123; \ldots, x_&#123;-1&#125; = 0, x_0 = 1.05^0, x_1 = 1.05^1, \ldots, x_i = 1.05^i, \ldots \&#125;.$$</code></p>
<p>而后，假设你每年存入 100 元，于是有冲激序列</p>
<p><code>$$y[n] = \&#123; \ldots, y_&#123;-1&#125; = 0, y_0 = 100, y_1 = 100, \ldots, y_i = 100, \ldots \&#125;.$$</code></p>
<p>于是，任意时刻的账户余额 $z[n] \overset{\text{def}}{&#x3D;} (x*y)[n]$ 是卷积。</p>
<table>
<thead>
<tr>
<th>time</th>
<th>$0$</th>
<th>$1$</th>
<th>$2$</th>
<th>$3$</th>
<th>$4$</th>
</tr>
</thead>
<tbody><tr>
<td>$y_0 &#x3D; 100$</td>
<td>$100$</td>
<td>$100\times 1.05^1$</td>
<td>$100\times 1.05^2$</td>
<td>$100\times 1.05^3$</td>
<td>$100\times 1.05^4$</td>
</tr>
<tr>
<td>$y_1 &#x3D; 100$</td>
<td>$0$</td>
<td>$100$</td>
<td>$100\times 1.05^1$</td>
<td>$100\times 1.05^2$</td>
<td>$100\times 1.05^3$</td>
</tr>
<tr>
<td>$y_2 &#x3D; 100$</td>
<td>$0$</td>
<td>$0$</td>
<td>$100$</td>
<td>$100\times 1.05^1$</td>
<td>$100\times 1.05^2$</td>
</tr>
<tr>
<td>$y_3 &#x3D; 100$</td>
<td>$0$</td>
<td>$0$</td>
<td>$0$</td>
<td>$100$</td>
<td>$100\times 1.05^1$</td>
</tr>
<tr>
<td>$y_4 &#x3D; 100$</td>
<td>$0$</td>
<td>$0$</td>
<td>$0$</td>
<td>$0$</td>
<td>$100$</td>
</tr>
</tbody></table>
<p>接下来，我们回过头观察 <code>$\sum_&#123;m = -\infty&#125;^&#123;+\infty&#125;x[m]\cdot y[n - m]$</code> 这个式子。</p>
<p>若以 $m$ 为「自变量」，则 $y[n - m]$ 相当于把 $y[m]$ 的图像左右翻转（这即是<strong>翻转</strong>的来源），然后再向右移动 $n$ 个单位。而当 $n$ 增大时，相当于 $x$ 不动而 $y$ 沿着轴线向右滑动。当 $x$ 和 $y$ 重叠时，计算重叠部分的乘积，然后加和得到最终结果。这个加和的过程，放在连续函数的情景下，就是积分了。将这个过程，制作成动态图如下。</p>
<p><img data-src="/uploads/images/MachineLearning/automatic_investment_plan.gif" alt="定投收益示意图"></p>
<h3 id="怎样卷？"><a href="#怎样卷？" class="headerlink" title="怎样卷？"></a>怎样卷？</h3><p>通过观察 <code>$\sum_&#123;m = -\infty&#125;^&#123;+\infty&#125;x[m]\cdot y[n - m]$</code>，我们已经知道了卷积是怎样翻转的，也知道卷积的积分从何而来。现在我们讨论关于卷积的终极问题：究竟要怎样才能「卷起来」？</p>
<p><img data-src="/uploads/images/MachineLearning/how_to_convolve.png" alt="怎样卷起来"></p>
<p>我们来看这张图。它的横轴和纵轴被替换成了 $m$ 和 $n - m$，恰好对应 <code>$\sum_&#123;m = -\infty&#125;^&#123;+\infty&#125;x[m]\cdot y[n - m]$</code> 中的 $x[m]$ 和 $y[n - m]$。途中有两条斜线，斜线经过的整数交点上画着小黑点。这些小黑点代表相应位置的 $x[m]\cdot y[n - m]$；而斜线则代表将这条斜线上所有小黑点的值相加。</p>
<p>不难发现，$m(n-m)$-二维平面上斜率为 $-1$ 的斜线族，其中每条这样的斜线（包括没有画出来的），都表示了一个卷积。特别地，斜线上每一个整数点的横纵坐标相加（即是 $m + (n - m)$）都是 $n$。因此，斜线对应的卷积是 $(x * y)[n]$。这样，我们就建立了斜线与卷积值之间的对应关系。</p>
<p>现在，把 $m(n-m)$-二维平面想象成一块无限薄的地毯。接下来，我们沿着斜率为 $-1$ 的直线方向，把地毯卷起来。这样，我们就将地毯卷成了一条直线。而这条直线上的每个点，都对应了原平面上的一条直线。也就是说，在「卷地毯」的过程中，原平面的直线纷纷坍缩成了一系列的点。而这些轴线与其上的点，正可作为是卷积 $(x*y)[n]$ 中 $n$ 所在的数轴。</p>
<p>这就是为什么我们说，卷积它真的可以「卷」了。</p>
<h3 id="二问卷积"><a href="#二问卷积" class="headerlink" title="二问卷积"></a>二问卷积</h3><p>至此，一维离散卷积相关的内容，我们就介绍完了。现在我们回过头来看看，在介绍一维离散卷积的过程中，卷积表现出了哪些特点。</p>
<p>我们是通过线性时不变的信号系统引出卷积的概念的。若仍以信号系统的说辞为例，则不难发现：</p>
<ul>
<li>一个脉冲激励可以影响到信号系统在若干时刻的输出；</li>
<li>从另一个角度，这也就是说，信号系统任意时刻的输出，取决于相关的多个冲激输入。</li>
</ul>
<p>也就是说，和一般的函数不同，信号系统的输入和输出不是「一对一」的关系，而是「多对多」的关系。我们在后续介绍卷积神经网络的时候，会看到这一特点的作用。</p>
<p>此外，仍以信号系统的说辞为例，我们也不难发现，系统的最终输出，一方面取决于输入的激励信号长什么样子，另一方面取决于冲激响应的模式。这两方面相互作用（就是卷积），最终决定了信号系统的输出。</p>
<p>在后续对一维离散卷积的观察中，我们发现，连续地求解多个卷积值的时候（即，求解 $n &#x3D; 0, 1, 2, \ldots$ 的卷积值时），我们实际上做的事情可以归纳成：</p>
<ul>
<li>翻转输入信号；</li>
<li>输入信号沿轴线向前滑动；</li>
<li>输入信号与冲激响应叠加的部分分别求积，然后相加。</li>
</ul>
<p>而实际上我们发现，在卷积的定义中，$x$ 和 $y$ 是地位等同的。这就是说，我们也完全可以选择翻转而后滑动冲激响应的模式，再去求积、叠加。此时，我们通常会把冲激响应称为「卷积核」，而把整个过程形象地称之为：滑动卷积核。</p>
<h2 id="二维离散卷积"><a href="#二维离散卷积" class="headerlink" title="二维离散卷积"></a>二维离散卷积</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>恭喜你，现在我们进入「高维宇宙」。</p>
<p>首先，让我们回顾一下一维离散卷积的定义。</p>
<p><code>$$(x*y)[n] \overset&#123;\text&#123;def&#125;&#125;&#123;=&#125; \sum_&#123;m = -\infty&#125;^&#123;+\infty&#125;x[m]\cdot y[n - m].$$</code></p>
<p>二维卷积的定义，在形式上和一维卷积完全一致——只需要将一维卷积中的变量 $m$, $n$ 从标量变成向量即可。当然，你也可以将向量的两个分量展开，记成标量形式。</p>
<p><code>$$ \begin&#123;aligned&#125; (x*y)[\vec n] \overset&#123;\text&#123;def&#125;&#125;&#123;=&#125;&#123;&#125;&amp; \sum_&#123;\vec m = (-\infty, -\infty)&#125;^&#123;(+\infty, +\infty)&#125;x[\vec m]\cdot y[\vec n - \vec m] \\ (x*y)[n_1, n_2] \overset&#123;\text&#123;def&#125;&#125;&#123;=&#125;&#123;&#125;&amp; \sum_&#123;m_1 = -\infty&#125;^&#123;+\infty&#125;\sum_&#123;m_2 = -\infty&#125;^&#123;+\infty&#125;x[m_1, m_2]\cdot y[n_1 - m_1, n_2 - m_2]. \end&#123;aligned&#125; $$</code></p>
<blockquote>
<p>类似地，你可以定义更高维的卷积。</p>
</blockquote>
<p>若仔细观察公式，不难发现，我们在二维卷积中遇到的问题，和在一维卷积中遇到的问题完全一致。二维卷积具有和一维卷积几乎完全相同的性质、特点、作用。和一维卷积一样，二维卷积也可以看做是加权平均的推广：以 $x$ 为权，以 <code>$(n_1, n_2)$</code> 位中心，将 $y$ 距离中心 <code>$(-m_1, -m_2)$</code> 位置的值乘上 $x$ 距离中心 <code>$(m_1, m_2)$</code> 的值，最后加到一起。</p>
<h3 id="图像的滤镜"><a href="#图像的滤镜" class="headerlink" title="图像的滤镜"></a>图像的滤镜</h3><p>在实际应用中，卷积核 $x$ 的有效部分总是有限的。例如，下图展示了一个 $3\times 3$ 的卷积核，在图像上的滑动。</p>
<p><img data-src="/uploads/images/MachineLearning/convolutions_on_images.gif" alt="二维卷积示意图"></p>
<p>值得一提的是，对于图像来说，这个过程实际就是 PhotoShop 等图像处理软件中的「滤镜」效果。比如，假设我们有一个 $3\times 3$ 的卷积核</p>
<p><code>$$ \begin&#123;bmatrix&#125; 1/9 &amp; 1/9 &amp; 1/9 \\ 1/9 &amp; 1/9 &amp; 1/9 \\ 1/9 &amp; 1/9 &amp; 1/9 \end&#123;bmatrix&#125;, $$</code></p>
<p>从直觉上分析，它将中心点附近的共 9 个点的像素值，平均到输出图像的中心像素点上；这实际上就是模糊效果对应的滤镜（box-blur）。又比如，假设我们有这样的卷积核</p>
<p><code>$$ \begin&#123;bmatrix&#125; 0 &amp; -1 &amp; 0  \\ -1 &amp; 5 &amp; -1 \\ 0 &amp; -1 &amp; 0  \\ \end&#123;bmatrix&#125;, $$</code></p>
<p>从直觉上分析，它加强了中心像素点的作用，同时减小了位于其上下左右的四个像素点对它的干扰；这实际上就是锐化效果对应的滤镜（sharpen）。又比如，假设我们有这样的卷积核</p>
<p><code>$$ \begin&#123;bmatrix&#125; -1 &amp; -1 &amp; -1 \\ -1 &amp; 8 &amp; -1  \\ -1 &amp; -1 &amp; -1 \\ \end&#123;bmatrix&#125;, $$</code></p>
<p>从直觉上分析，只有当中心像素点原本的像素值和周围 8 个像素点的值差距很大时，这个卷积核的输出，才会明显地不等于 0；因此，这实际上就是边缘检测对应的滤镜（edge detect）。</p>
<p>我们将上述三个矩阵以 Python 实现出来，就能看到它们的效果了。（参见：<a href="/2017/08/06/pil-tutorial-pixel-operations-and-image-filter/#%E5%AE%9E%E9%99%85%E6%93%8D%E4%BD%9C%E7%9C%8B%E7%9C%8B%E2%80%94%E2%80%94%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%A7%AF%E6%BB%A4%E9%95%9C">PIL 简明教程 - 像素操作与图像滤镜</a>）其效果如下图所示。</p>
<p><img data-src="/uploads/images/MachineLearning/image_filter.png" alt="Python 实现的图像滤镜"></p>
<h3 id="三问卷积"><a href="#三问卷积" class="headerlink" title="三问卷积"></a>三问卷积</h3><p>又到了思考问题的<del>贤者</del>时间。</p>
<p>在介绍一维卷积的过程中，我们已经讨论了卷积本身具有的特点。但是，也留下了一个问题：卷积在抽象上，到底有什么意义呢？</p>
<p>站在人类的角度，我们先入为主地将上面 3 个示例的卷积核当做了「滤镜」。然而，事情真的是这样吗？如果我们忘记「滤镜」这一先验知识，那么我们可能会把这件事情，简单地以更抽象的方式描述为「卷积核<strong>处理</strong>图形」。没错，这仅仅是一个「处理」过程而已。现在我们回想一下，环境中的真实景象，也是经过我们的大脑处理之后，在脑海里形成实际的画面的。若然你知道，同一个真实景象，在不同生物的眼里是不一样的。那么你就不难发现，不同的生物，因其进化路径不同，大脑对环境真实景象的处理也不同，因而脑海中看到的景象也就不同。这与我们用不同的滤镜处理图像，得到不同的滤镜结果，何其相似？</p>
<p>刚才我们说到不同生物眼里的世界是不一样的。那么，更深入地理解一下这份不同，我们会否领会到更多的东西呢？</p>
<p>比如，我们可以思考：为什么自然选择会让不同的生物看到不同的景象？答案其实很简单：因为适者生存。蛇类的眼睛，按照人类的意识，几乎不能视物；然而因为经常需要夜间活动，所以蛇能够以红外的方式「看到」这个世界。青蛙的眼睛，难以察觉到静止的事物；然而因为它只对「会动的虫子」感兴趣，所以青蛙具有奇佳的动态视觉。站在更广的时间维度上，我们可以这样回答这个问题：对于具体的某种生物来说，因其生存需要，它只对某种形式的视觉效果感兴趣，因而其视觉处理系统进化成了当前的模样。简而言之，不同的生物，看待世界的方式，有不同的侧重点，因而将同一个真实景象处理成了不同的模样。</p>
<p>这里我们对生物的视觉效果进行了展开分析。这不是我要「跨界」当「神棍」，而是想以一种直觉的方式，以普遍的现象为对比，试着能够更好地理解卷积的意义。</p>
<p>至此，我们可以比较容易地制作出一张对应的表格。</p>
<table>
<thead>
<tr>
<th>卷积</th>
<th>生物视觉</th>
<th>机器学习领域的意义</th>
</tr>
</thead>
<tbody><tr>
<td>滑动卷积核</td>
<td>视觉系统处理外界光信号</td>
<td>读入并处理结构化的特征</td>
</tr>
<tr>
<td>卷积核处理的结果</td>
<td>脑海中形成的视觉成像</td>
<td>卷积处理的结果</td>
</tr>
<tr>
<td>不同的卷积核</td>
<td>观察世界的不同方式、不同侧重点</td>
<td>不同角度的高维特征信号</td>
</tr>
</tbody></table>
<p>这也就是说，特定的卷积核，能够从<strong>若干相关特征信号</strong>（通常是相邻位置的特征信号）中<strong>以特定的方式</strong>抽取新的高维特征。</p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>有了这些关于卷积的知识基础，现在我们可以讨论卷积神经网络了。我们假设你已经对神经网络有所了解，因此就不去从感知机开始，逐步地介绍了。</p>
<h3 id="图片识别任务"><a href="#图片识别任务" class="headerlink" title="图片识别任务"></a>图片识别任务</h3><blockquote>
<p>这个例子，来自于 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27642620">YJango的卷积神经网络——介绍</a>。</p>
</blockquote>
<p>在介绍二维离散卷积的时候，我们以图片为例。这是因为，图片是天然的二维像素矩阵组成的数据形式（RBG 三通道即是 3 个矩阵）。因此，专业里我们也以图片识别任务为例，展开对卷积神经网络的介绍。</p>
<p><img data-src="/uploads/images/MachineLearning/pattern_reg_target.png" alt="识别横折"></p>
<p>如上图。每一个 $4 \times 4$ 的方块，都表示一张图片。在我们的表示中，黄色的圆圈表示空无一物的底色；黑色的圆圈，表示有内容的笔画。现在，我们要识别上图顶部的「横折」这一笔画。显而易见，上图下半部分的 6 张图片，内里都包含了横折。</p>
<h3 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h3><p>对于这样的图片识别任务，使用深度前馈神经网络来解决，当然是可以的。</p>
<p><img data-src="/uploads/images/MachineLearning/pattern_reg_dnn.png" alt="深度神经网络（DNN）图例"></p>
<p>如上图。为了解决这样的问题，我们首先需要将原始图片制成一个能用向量表示出来的数据形式。最简单的办法，就是将二维的图像，逐行地展开。如此，我们就从 <code>object</code> 得到了输入层 <code>input</code>。接下来，我们就可以把输入层链接到隐藏层当中，经过逐层地全连接，得到最终输出 <code>output</code>。通常来说，这个最终输出，是神经网络给出的概率。这个概率描述，神经网络认为当前图片中，包含「横折」的概率。</p>
<p>经过大量的训练，这样的神经网络可以很好地完成识别任务。然而，这样的网络设计，也可能存在一些问题。</p>
<p><img data-src="/uploads/images/MachineLearning/pattern_reg_cases.png" alt="图片识别样本"></p>
<p>如上图。假设左侧的 4 张图片，是我们标注好的训练样本。经过训练之后，我们的神经网络应当已经具有一定的能力，尝试识别图片中是否存在「横折」这一笔画。然而，由于训练神经网络时的输入样本十分有限，我们的神经网络可能并不认得右侧的样本。特别地，在我们的神经网络示意图中，左上角的横折和位于中间的横折是完全不同的两个向量。因此，我们得到的神经网络模型，很可能无法给出对右侧未知样本的准确预测。</p>
<p>那么，怎么办呢？</p>
<p>最最简单容易想到的办法，就是增加训练时的训练样本。若然我们能够让样本覆盖所有情况，那么训练得到的神经网络自然就可以识别所有的情况，并给出结论了。不过，最简单容易想到的解法，往往暗含各种各样的问题。首先，我们的图片识别任务中，图片都是 $4 \times 4$ 的小型图片。对这类图片，穷举所有可能，其总数也只有 $2^{16}$ 张。对于这种类型的问题，穷举所有情况，大致是没有问题的。然而，实际生产中，我们遇到的问题，其复杂度要远远高于现在我们所言的「玩具问题」。在实际问题中，我们不可能让样本覆盖所有情形。另一方面，若是简单粗暴地扩增样本容量，就失去了模型「预测」的意义了。换而言之，这就不是我们追求的高可泛化的模型了。</p>
<h3 id="表意的平移不变性——对问题的深入思考"><a href="#表意的平移不变性——对问题的深入思考" class="headerlink" title="表意的平移不变性——对问题的深入思考"></a>表意的平移不变性——对问题的深入思考</h3><p>扩大训练集的解法，当然也是一个办法。在实际生产中，有些时候也确实需要扩大训练集，以解决一些欠拟合的问题。然而，正如任何定理都有其适用范围，我们也需要斟酌扩大训练集在当前任务中是否合适。显然，有上面的分析，在当前任务中，这不是个好办法。</p>
<p>那么，问题出在哪里呢？或者说，我们应当在哪个方向前进，以便解决这个问题呢？在上面的分析中，我们有提到一句话：「在我们的神经网络示意图中，左上角的横折和位于中间的横折是完全不同的两个向量」。我想，若你足够敏感，应该能意识到什么。</p>
<p>不好。这很不好。在表意上，位于图片左上角的横折之于位于图片中间的横折没有什么差别。也就是说，在图片上任意平移横折的位置，其表意不发生变化。我们称之为表意的平移不变性。然而，在我们的神经网络中，这两个横折在输入层的表现居然没有什么共同点。显而易见，这是不合理的。因此，在遇到的这个问题中，我们首先应该考虑的，不是扩增训练集，而是应当考虑我们神经网络是否足够好地适应当前的问题。</p>
<p>那么，我们的神经网络中，问题出在哪里呢？</p>
<p>首先，我们的神经网络是针对每个像素的具体情况进行训练的。其次，图片上的区域各自为政，没有关联。也就是说，我们的神经网络，很难捕捉到相邻区域中几个像素点（特征值）的结构信息。另一方面，我们的神经网络，也没有以一种一致地视角，去看待每一个局部的结构。这样一来，我们的神经网络就可能会把位于左上角的横折与位于中间的横折，当成两个完全没有关联的图形。这显然是不合适的。</p>
<h3 id="引入卷积"><a href="#引入卷积" class="headerlink" title="引入卷积"></a>引入卷积</h3><p>至此，就轮到卷积出场<del>拯救世界</del>了。</p>
<p>我们回顾一下简单的前馈神经网络在当前任务中遇到的问题：无法一致地捕捉局部的结构信息。我们再来回想一下卷积的特点：以一个固定的卷积核，收集相邻特征信号的信息，加权平均得到卷积值。啊！卷积的这些特点，不就正好弥补了当前前馈神经网络的不足吗？</p>
<p>于是，我们可以设计出这样的网络结构。</p>
<p><img data-src="/uploads/images/MachineLearning/pattern_reg_cnn.png" alt="卷积神经网络（CNN）图例"></p>
<p>如上图。首先，我们用一个固定的 $2 \times 2$ 的卷积核作为窗口，逐个像素地扫描原图片。这样一来，我们可以得到 $3 \times 3$ 的卷积结果，称为 <code>convolved feature</code>。而后，和我们在前馈神经网络中做的一样，我们将 <code>convolved feature</code> 展开，作为输入层，链接其背后的隐藏层，并最终得到输出。</p>
<p>在这个过程中，神经网络的参数，除了隐藏层中的各个神经元上的参数，还有卷积核的具体内容。也就是说，卷积核的大小是固定的，但是它长什么样子，是需要具体训练的。</p>
<p>这样引入了卷积的神经网络，就是卷积神经网络（Convolution Neural Network, CNN）了。当然，在实际使用中，还常常引入名为池化（Pooling）的技术，这里按下不表。</p>
<h3 id="不变性的讨论"><a href="#不变性的讨论" class="headerlink" title="不变性的讨论"></a>不变性的讨论</h3><p>有了卷积，我们的神经网络就能一致地去捕捉输入信号局部的结构信息。特别地，由于卷积核在不同位置上是共享的，所以笔画的平移在神经网络看来，就不影响表意了。因此我们说，卷积神经网络满足了平移不变性。</p>
<p>那么，是否还有其它的不变性呢？当然是有的。</p>
<p>比如，我们现在的横折由 $3$ 个像素在 $2 \times 2$ 的局部中组成。那么，若是将它放大，在 $3 \times 3$ 的局部中，用 $5$ 个像素去组成横折，是否也可以呢？答案是显而易见的：大猫也是猫，大狗也是狗。这种现象，我们称之为缩放不变性。那么，当前的卷积神经网络，是否能解决这样的问题呢？我们说，不能。因为我们当前使用的卷积核是 $2 \times 2$ 的，它无法去捕捉 $3 \times 3$ 的局部结构中的完整信息。因此，当前的卷积神经网络，没有满足缩放不变性。若要满足缩放不变性，我们可以考虑用不同大小的卷积核，分别处理原图像；或者，可以考虑在卷积层的基础上，再用卷积处理一次。</p>
<p>又比如，假设我们不识别笔画，我们识别图片中的铅笔。在图片中，除了说铅笔可大可小，位置上可以在图片上游走，铅笔还可能以不同角度出现——横着放的、竖着放的、斜着放的。但不论铅笔如何摆放，它都是铅笔。这种现象，我们称之为旋转不变性。不过，很遗憾，由于卷积的特性所限，我们无法简单地用卷积，让神经网络满足旋转不变性。</p>
<h3 id="卷积神经网络直觉上的优势"><a href="#卷积神经网络直觉上的优势" class="headerlink" title="卷积神经网络直觉上的优势"></a>卷积神经网络直觉上的优势</h3><p>上面我们讨论了卷积的特点。因此，我们不难总结卷积神经网络的一些优势。</p>
<ul>
<li>适用于相关元素（特别是相邻元素）中存在结构特征的情况；</li>
<li>适用于上述结构可能出现在不同位置的情况。</li>
</ul>
<p>现在，我们考虑一下分类问题。对于分类问题来说</p>
<ul>
<li>在分割线（可能是超平面、超曲面）附近，样本往往存在特定的结构特征；</li>
<li>输入的样本，可能位于分割线的不同位置，因此上述结构也可能出现在分割线的不同位置。</li>
</ul>
<p>分类问题的这样的特点，恰恰符合了卷积神经网络的优势。因此，人们常常偏向于认为：「卷积神经网络可以在分类问题上表现得好」。</p>

    </div>

    
    
    <div class="post-widgets">
      <div id="needsharebutton-postbottom">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    </div>
        <div class="reward-container">
  <div>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/reward/wechatpay-cropped.png" alt="Liam Huang 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/reward/alipay-cropped.png" alt="Liam Huang 支付宝">
        <p>支付宝</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/reward/paypal.jpeg" alt="Liam Huang 贝宝">
        <p>贝宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Liam Huang
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://liam.page/2017/07/27/convolutions-and-convolution-neural-network/" title="谈谈离散卷积和卷积神经网络">https://liam.page/2017/07/27/convolutions-and-convolution-neural-network/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Convolutions/" rel="tag"># Convolutions</a>
              <a href="/tags/CNN/" rel="tag"># CNN</a>
              <a href="/tags/Neural-Network/" rel="tag"># Neural Network</a>
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2017/07/23/global-scale-for-TikZ-picture/" rel="prev" title="在 LaTeX 中同步缩放 TikZ 与其中的 node">
      <i class="fa fa-chevron-left"></i> 在 LaTeX 中同步缩放 TikZ 与其中的 node
    </a></div>
      <div class="post-nav-item">
    <a href="/2017/07/30/define-a-new-command-with-different-amount-of-parameters-in-LaTeX/" rel="next" title="LaTeX 黑魔法（三）：定义参数变长的命令">
      LaTeX 黑魔法（三）：定义参数变长的命令 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9D%E8%AF%86%E5%8D%B7%E7%A7%AF"><span class="nav-number">1.</span> <span class="nav-text">初识卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E9%97%AE%E5%8D%B7%E7%A7%AF"><span class="nav-number">1.1.</span> <span class="nav-text">一问卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BD%A2%E5%BC%8F%E5%AE%9A%E4%B9%89"><span class="nav-number">1.2.</span> <span class="nav-text">形式定义</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E7%BB%B4%E7%A6%BB%E6%95%A3%E5%8D%B7%E7%A7%AF"><span class="nav-number">2.</span> <span class="nav-text">一维离散卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E6%97%B6%E4%B8%8D%E5%8F%98%E7%B3%BB%E7%BB%9F"><span class="nav-number">2.1.</span> <span class="nav-text">线性时不变系统</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%B2%E6%BF%80%E5%92%8C%E5%93%8D%E5%BA%94"><span class="nav-number">2.2.</span> <span class="nav-text">冲激和响应</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%9E%E7%BB%AD%E5%86%B2%E6%BF%80%E7%9A%84%E5%93%8D%E5%BA%94"><span class="nav-number">2.3.</span> <span class="nav-text">连续冲激的响应</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A6%BB%E6%95%A3%E5%8D%B7%E7%A7%AF"><span class="nav-number">2.4.</span> <span class="nav-text">离散卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E6%8A%95%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-number">2.5.</span> <span class="nav-text">定投的例子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%8E%E6%A0%B7%E5%8D%B7%EF%BC%9F"><span class="nav-number">2.6.</span> <span class="nav-text">怎样卷？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E9%97%AE%E5%8D%B7%E7%A7%AF"><span class="nav-number">2.7.</span> <span class="nav-text">二问卷积</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E7%BB%B4%E7%A6%BB%E6%95%A3%E5%8D%B7%E7%A7%AF"><span class="nav-number">3.</span> <span class="nav-text">二维离散卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89"><span class="nav-number">3.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E7%9A%84%E6%BB%A4%E9%95%9C"><span class="nav-number">3.2.</span> <span class="nav-text">图像的滤镜</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%89%E9%97%AE%E5%8D%B7%E7%A7%AF"><span class="nav-number">3.3.</span> <span class="nav-text">三问卷积</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.</span> <span class="nav-text">卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E7%89%87%E8%AF%86%E5%88%AB%E4%BB%BB%E5%8A%A1"><span class="nav-number">4.1.</span> <span class="nav-text">图片识别任务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.2.</span> <span class="nav-text">前馈神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%A8%E6%84%8F%E7%9A%84%E5%B9%B3%E7%A7%BB%E4%B8%8D%E5%8F%98%E6%80%A7%E2%80%94%E2%80%94%E5%AF%B9%E9%97%AE%E9%A2%98%E7%9A%84%E6%B7%B1%E5%85%A5%E6%80%9D%E8%80%83"><span class="nav-number">4.3.</span> <span class="nav-text">表意的平移不变性——对问题的深入思考</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%95%E5%85%A5%E5%8D%B7%E7%A7%AF"><span class="nav-number">4.4.</span> <span class="nav-text">引入卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E5%8F%98%E6%80%A7%E7%9A%84%E8%AE%A8%E8%AE%BA"><span class="nav-number">4.5.</span> <span class="nav-text">不变性的讨论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9B%B4%E8%A7%89%E4%B8%8A%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="nav-number">4.6.</span> <span class="nav-text">卷积神经网络直觉上的优势</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Liam Huang"
      src="/images/avatar/avatar-3.jpg">
  <p class="site-author-name" itemprop="name">Liam Huang</p>
  <div class="site-description" itemprop="description">虚室生白，吉祥止止</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">404</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">773</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-fw fa-rss"></i>RSS</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://dennybritz.com/" title="https:&#x2F;&#x2F;dennybritz.com&#x2F;" rel="noopener" target="_blank">Denny</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://chattymoney.com/" title="http:&#x2F;&#x2F;chattymoney.com&#x2F;" rel="noopener" target="_blank">ChattyMoney</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://qixinbo.info/" title="http:&#x2F;&#x2F;qixinbo.info&#x2F;" rel="noopener" target="_blank">XinboQi</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://ionizing.page/" title="https:&#x2F;&#x2F;ionizing.page&#x2F;" rel="noopener" target="_blank">ChenLinjie</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://yihui.name/" title="https:&#x2F;&#x2F;yihui.name&#x2F;" rel="noopener" target="_blank">Yihui</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.felixc.at/" title="https:&#x2F;&#x2F;blog.felixc.at&#x2F;" rel="noopener" target="_blank">Felix</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://wyydsb.xin/" title="https:&#x2F;&#x2F;wyydsb.xin&#x2F;" rel="noopener" target="_blank">Gunjianpan</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://joselynzhao.top/" title="https:&#x2F;&#x2F;joselynzhao.top&#x2F;" rel="noopener" target="_blank">JoselynZhao</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.dang.fan/" title="https:&#x2F;&#x2F;blog.dang.fan&#x2F;" rel="noopener" target="_blank">DangFan</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://harrychen.xyz/" title="https:&#x2F;&#x2F;harrychen.xyz" rel="noopener" target="_blank">HarryChen</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.echen.me/" title="https:&#x2F;&#x2F;blog.echen.me&#x2F;" rel="noopener" target="_blank">EdwinChen</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://fengweiustc.github.io/" title="https:&#x2F;&#x2F;fengweiustc.github.io&#x2F;" rel="noopener" target="_blank">WayneFung</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.cyningsun.com/" title="https:&#x2F;&#x2F;www.cyningsun.com&#x2F;" rel="noopener" target="_blank">CyningSun</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://0o0blog.com/" title="https:&#x2F;&#x2F;0o0blog.com&#x2F;" rel="noopener" target="_blank">Francis</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2013 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liam Huang</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">1.5m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">45:11</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>


<script>
  window.addEventListener('DOMContentLoaded', function() {
    
    
    var intervalTotalViews = setInterval(fixTotalViews, 100);
    var offsetTotalViews   = parseInt(100000);
    function fixTotalViews() {
      if (document.getElementById('busuanzi_container_site_pv').style.display != "none") {
        clearInterval(intervalTotalViews);
        var el = document.getElementById("busuanzi_value_site_pv");
        var value = parseInt(el.innerHTML) + offsetTotalViews;
        el.innerHTML = '' + value;
      }
    }
    
  });
</script>










      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/js/local-search.js"></script>














<script type="text/javascript">
var crashSwitched = false;
var originalTitle = document.title;
var titleTime;
document.addEventListener('visibilitychange', function () {
  if (document.hidden) {
    if (Math.random() < parseFloat(0.25)) {
      crashSwitched = true;
      document.title = '╭(°A°`)╮ 页面崩溃啦~' + originalTitle;
      clearTimeout(titleTime);
    }
  } else {
    if (crashSwitched == true) {
      crashSwitched = false;
      document.title = '(ฅ>ω<*ฅ) 咦，又好了~' + originalTitle;
      titleTime = setTimeout(function () {
        document.title = originalTitle;
      }, 2000);
    }
  }
});
</script>



    <div id="pjax">
  

  
      

<script>
    document.querySelectorAll('code').forEach(code => {
      const text = code.innerHTML;
      // is_inline_math  = /^\$(.*)\$$/.exec(text) || /^\\\((.*)\\\)$/.exec(text);
      // is_display_math = /^\$\$(.*)\$\$$/ms.exec(text) || /^\\begin\{.+\}(.*)\\end\{.+\}/ms.exec(text);
      if (/^\$\$(.*)\$\$$/ms.exec(text) || /^\\begin\{.+\}(.*)\\end\{.+\}/ms.exec(text)) {
        code.outerHTML = "<span class='theme_next_mathjax_display has-jax'>" + text + "</span>";
      } else if (/^\$(.*)\$$/.exec(text) || /^\\\((.*)\\\)$/.exec(text)) {
        code.outerHTML = "<span class='theme_next_mathjax_inline has-jax'>"  + text + "</span>";
      }
    });
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/ams'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['ams']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://liam.page/2017/07/27/convolutions-and-convolution-neural-network/',]
      });
      });
  </script>

  <script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-needmoreshare2@1/needsharebutton.min.js"></script>
  <script>
      pbOptions = {};
        pbOptions.iconStyle = "default";
        pbOptions.boxForm = "horizontal";
        pbOptions.position = "topCenter";
        pbOptions.networks = "Wechat,Weibo,renren,Twitter,Facebook,Douban,QQZone,Evernote,Mailto";
      new needShareButton('#needsharebutton-postbottom', pbOptions);
  </script>
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'a3d846b3392f468b5746',
      clientSecret: '61bc947cad0ec7078e800e05b1e3c78b763b2c55',
      repo        : 'liam0205.github.io',
      owner       : 'Liam0205',
      admin       : ['Liam0205'],
      id          : '63d1dda790a1d976ab0605fd00a680c6',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

    </div>
</body>
</html>
